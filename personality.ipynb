{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU,Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.contrib import learn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, TREC=False):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s \", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have \", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not \", string)\n",
    "    string = re.sub(r\"\\'re\", \" are \", string)\n",
    "    string = re.sub(r\"\\'d\" , \" would \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will \", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "#   string = re.sub(r\"[^\\d\\.]\",\"\",string)\n",
    "#   string = re.sub(r\"[a-zA-Z]{4,}\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip() if TREC else string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #AUTHID                                               TEXT cEXT  \\\n",
       "0  1997_504851.txt  Well, right now I just woke up from a mid-day ...    n   \n",
       "1  1997_605191.txt  Well, here we go with the stream of consciousn...    n   \n",
       "2  1997_687252.txt  An open keyboard and buttons to push. The thin...    n   \n",
       "3  1997_568848.txt  I can't believe it!  It's really happening!  M...    y   \n",
       "4  1997_688160.txt  Well, here I go with the good old stream of co...    y   \n",
       "\n",
       "  cNEU cAGR cCON cOPN  \n",
       "0    y    y    n    y  \n",
       "1    n    y    n    n  \n",
       "2    y    n    y    y  \n",
       "3    n    y    y    n  \n",
       "4    n    y    n    y  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('essays.csv',encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=df['TEXT']\n",
    "#raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>Charged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>march</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>august</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0   march      0             0        0     0    0         0         1   \n",
       "1  august      0             0        0     0    0         0         1   \n",
       "2     ago      0             0        0     0    0         0         0   \n",
       "3     mar      0             0        0     0    0         1         0   \n",
       "4     vie      0             0        0     0    0         0         0   \n",
       "\n",
       "   sadness  surprise  trust  Charged  \n",
       "0        0         0      0        1  \n",
       "1        0         0      0        1  \n",
       "2        0         0      0        0  \n",
       "3        0         0      0        1  \n",
       "4        0         0      0        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotionfile=pd.read_csv('Emotion_Lexicon.csv')\n",
    "emotionfile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emontial=emotionfile[emotionfile['Charged']!=0]['Words'].tolist()\n",
    "#emontial\n",
    "\n",
    "#emotionfile['sum']=emotionfile.sum(axis=1)\n",
    "#emontial=emotionfile[emotionfile['sum']!=0]['Words'].tolist()\n",
    "#emontial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[]\n",
    "sentence_length=[]\n",
    "unemontial=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6468"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emontial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(raw)):\n",
    "    essay=re.split(r'[.?]',raw[i])\n",
    "    sentences=[]\n",
    "    for sent in essay:\n",
    "        sent=clean_str(sent)\n",
    "        sent=sent.split(' ')\n",
    "        if(len(sent)>150):\n",
    "            newsent=[]\n",
    "            splits=int(np.floor(len(sent)/20))\n",
    "            for index in range(splits):\n",
    "                            newsent.append(' '.join(sent[index*20:(index+1)*20]))\n",
    "            if len(sent)>splits*20:\n",
    "                            newsent.append(' '.join(sent[splits*20:]))\n",
    "            for s in newsent:\n",
    "                n=0\n",
    "                s=s.split(' ')\n",
    "                for word in s:\n",
    "                    if word in emontial:\n",
    "                        n=n+1\n",
    "                if n!=0:\n",
    "                    sentence_length.append(len(s))\n",
    "                    s=' '.join(s)\n",
    "                    sentences.append(s)\n",
    "                    \n",
    "        else:\n",
    "            n=0\n",
    "            for word in sent:\n",
    "                if word in emontial:\n",
    "                    n=n+1\n",
    "            if n!=0:\n",
    "                sentence_length.append(len(sent))\n",
    "                sent=' '.join(sent)\n",
    "                sentences.append(sent)\n",
    "    if sentences:\n",
    "        doc.append(sentences)\n",
    "    else:\n",
    "        unemontial.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"he wants to go to a hotel , but i know i have to babysit and i'm not sure exactly how i will tell my dad to pick us up from here\",\n",
       " 'my birthday is this weekend , and it does not really even feel like it',\n",
       " 'i do not really mind , but i have a feeling brady probably wo not do much for it',\n",
       " 'he says he will pay for the hotel , but i do not really want that for my birthday',\n",
       " \"i'ts just not htat important to me\",\n",
       " 'i feel guilty today for lying to my dad about gettin gout of the tip program',\n",
       " 'i know that sometimes i have to lie about things like that in order for them to understand',\n",
       " \"i have not heard from my mom so i'm not sure if she 's mad at me or not\",\n",
       " 'i wish brady would pay more attention',\n",
       " 'he tries so hard , but it does not seem like he has the chance to do so',\n",
       " \"i'm feeling that although i do not weigh that much my weight is getting out of control\",\n",
       " 'kristen and i went shopping yesterday',\n",
       " 'i really do not plan on shopping anymore',\n",
       " \"i'm surprised i did not get one\",\n",
       " \"he 's concerned that i tell her too mucha bout our personal life , but i really do not\",\n",
       " 'dana kind of pisses me off',\n",
       " 'why is everyone so negative about a caring relationship',\n",
       " \"she 's probably just jealous , she has not been able to keep a guy at all for a long time\",\n",
       " \"i'm concerned for her because i do not want her to be doing drugs anymore , but she probably is\",\n",
       " 'she has too much sex too',\n",
       " \"it 's hard to not be concerned with what your friends are doing\",\n",
       " \"kristen is surprisingly very understanding to everything that 's going on in my life\",\n",
       " \"i was not sure what to expect since she 's friends with kristin , hayley etc\",\n",
       " 'they apparently are not waht i expected them to be',\n",
       " 'they play if off to be all god like , when in all realtiy they judge pretty much everyone',\n",
       " \"although i'm not that religious , i do care about god and do not think that those kinds of behavior are fair\",\n",
       " 'kris was kind of weird about the whole boyfriend thing',\n",
       " 'i hope that brady and i last',\n",
       " 'i would do anything to spend the rest of my life with him',\n",
       " 'i ca not imagine having to go to school with him',\n",
       " 'his roommates are not as crazy as i thought , but bad enough',\n",
       " 'i jut hope that he does not get heavy into drinking , a nd i do not assume he will',\n",
       " 'sarah is a bitch , i ca not believe she comes off like taht',\n",
       " 'my mom is making a big deal about me not calling , but in all honesty i just do not care',\n",
       " \"it 's probably because i'm about to start my period\",\n",
       " 'maybe i will skip it this month , in fact i probably will',\n",
       " 'i do not usually have allergies , but for some reason my eyes keep tearing',\n",
       " 'the eyes watering is a little embarrassing',\n",
       " 'i kind of wish i woul dhave rushed to be in a sorority',\n",
       " 'i feel as though i ahve no friends because i have lost them to brady',\n",
       " 'should i go to florida with my parents during spring break or should i spend time with brady instead',\n",
       " 'maybe i could make money babysitting',\n",
       " 'i hope my parents are not mad because i want to babysit on saturday instead ofdoing my birthday thing',\n",
       " 'i do not see why it would be that big of a deal , but apparently it is',\n",
       " \"i kind of want to go tanning , but it 's not good with my current skin situation and whatnot\",\n",
       " \"i'm really hungry too\",\n",
       " \"i'm not sure what i'm going to eat , but i wish kristen would get here so taht we can go eat\",\n",
       " 'that would be really good',\n",
       " 'i almost feel as though if i marry brady will accomplish everything',\n",
       " 'i want so badly to be with him forever',\n",
       " \"i'm glad that i have gotten to know hhis family a lot better\",\n",
       " 'his mom is honestly really nice to me , and i do not ahve a problem with her anymore',\n",
       " \"she 's has it really good\",\n",
       " 'they are always trying to save money when in reality being multi millionaires does not mean that you should only do that',\n",
       " \"perhaps it 's a shock from the inheritance or something\",\n",
       " \"i'm a little tired of writing\",\n",
       " \"it 's probably because i'm hungry\",\n",
       " \"i'm a little confused as to what i should study here at school\",\n",
       " 'i have had an interst in psychology , but do i really want to do that for the rest of my life',\n",
       " 'yay , kristen got back and we can go eat now',\n",
       " 'my time is almost over']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have just gotten off the phone with brady. I'm trying to decide what exacly we will do  this weekend. he wants to go to a hotel, but I know I have to babysit and I'm not sure exactly  how I will tell my dad to pick us up from here. My birthday is this weekend, and it doesn't really  even feel like it. I don't really mind, but I have a feeling Brady probably won't do much for it. I  almost feel like I'm always doing something for him. I should, I really care about him. He says  he will pay for the hotel, but I don't really want that for my birthday. I'ts just not htat important  to me. I feel guilty today for lying to my dad about gettin gout of the TIP program. I know that  sometimes I have to lie about things like that in order for them to understand. I haven't heard  from my mom so I'm not sure if she's mad at me or not. She probably doesn't care, but when  she does find out she will freak. I wish Brady would pay more attention. He tries so hard, but it  doesn't seem like he has the chance to do so. I need to work out. I'm feeling that although I  don't weigh that much my weight is getting out of control. Perhaps I'm overreacting. Kristen  and I went shopping yesterday. I really don't plan on shopping anymore. Yikes. We don't really  need anything anyways. I need a strapless bra though. I'm surprised I didn't get one. oh well. I wish I could figure out what to do on Friday with Brady. I want Kristen to be able to hang out  with us too. He's concerned that I tell her too mucha bout our personal life, but I really don't. Dana kind of pisses me off. Why is everyone so negative about a caring relationship?  She's  probably just jealous, she hasn't been able to keep a guy at all for a long time. I'm concerned  for her because I don't want her to be doing drugs anymore, but she probably is. She has too much  sex too. lol. It's hard to not be concerned with what your friends are doing. Kristen is  surprisingly very understanding to everything that's going on in my life. She's caring and funny  to be around. I wasn't sure what to expect since she's friends with Kristin, Hayley etc. They  apparently aren't waht I expected them to be. They play if off to be all God-like, when in all  realtiy they judge pretty much everyone. I don't think that is right. Although I'm not that  religious, I do care about God and don't think that those kinds of behavior are fair. I'm looking  att he picture of Brady and me. Kris was kind of weird about the whole boyfriend thing. I hope  that brady and I last. I would do anything to spend the rest of my life with him. I care about  him so much, and the distance hurts. I think it isi probably for the best though. I can't imagine  having to go to school with him. His roommates aren't as crazy as I thought, but bad enough. I  jut hope that he doesn't get heavy into drinking,a nd I don't assume he will. Sarah is a bitch, I  can't believe she comes off like taht. ugh, it really bothers me. I wonder what it's like at home  without me there. My mom is making a big deal about me not calling, but in all honesty I just  don't care. I don't know why I'm so unfeeling lately. It's probably because I'm about to start my  period. maybe I'll skip it this month, in fact I probably will. I don't usually have allergies, but for  some reason my eyes keep tearing. It's either my contacts or allergies. I just changed my  contacts though. I really have to pee. The eyes watering is a little embarrassing. Sometimes I  wish I coiuld do more. I kind of wish I woul dhave rushed to be in a sorority. I feel as though I  ahve no friends because I have lost them to brady. Should I go to florida with my parents during  spring break or should I spend time with Brady instead?  Maybe I could make money  babysitting. That would be nice. I hope my parents aren't mad because I want to babysit on  Saturday instead ofdoing my birthday thing. I don't see why it would be that big of a deal, but  apparently it is. I kind of want to go tanning, but it's not good with my current skin situation and  whatnot. that reminds me of richard rush. I wonder how he is doing. Spoiled brat. I'm really  hungry too. I'm not sure what I'm going to eat, but I wish Kristen would get here so taht we can  go eat. That would be really good. lol. I'm hoping that I won't look old when I'm 25 or so. Heck, I don't even know what I want to do with my life. I almost feel as though if I marry Brady  will accomplish everything. I want so badly to be with him forever. NO one understand me like  he does. I wish I had saved myself for him, like he did for me. How is he able to only commit  to me?  I must care about him so much that he feels like he should. I'm glad that I've gotten to  know hhis family a lot better. his mom is honestly really nice to me, and I don't ahve a problem  with her anymore. It would be awesome to be ilke paris hilton. She's has it really good. I  wonder what it would be like if my parents lived more like they should. They are always trying  to save money when in reality being multi-millionaires does not mean that you should only do  that. Perhaps it's a shock from the inheritance or something. I don't know. It's confusing to  me. Ok, this is getting old. I'm a little tired of writing. It's probably because I'm hungry. I'm a little  confused as to what I should study here at school. I've had an interst in psychology, but do I  really want to do that for the rest of my life. I know it doesn't just end here, but I feel as though  there aren't many options when it comes to what I can do with my life. yay, kristen got back and  we can go eat now. My time is almost over. I'm about to pee in my pants too. AHHHHH!!!!! some people try to hard \""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[2466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentencelength=max(sentence_length)\n",
    "max_sentencelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for essay in doc:\n",
    "    for sentence in essay:\n",
    "        b.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73334"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well , right now i just woke up from a mid day nap'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-50-aec4645bb12a>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3, ...,   0,   0,   0],\n",
       "       [ 13,  14,  15, ...,   0,   0,   0],\n",
       "       [  4,  30,  31, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 32, 127, 123, ...,   0,   0,   0],\n",
       "       [  1,  13,  14, ...,   0,   0,   0],\n",
       "       [ 59,   4, 251, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_sentencelength)\n",
    "vocab_processor.fit_transform(b)\n",
    "np.array(list(vocab_processor.transform(doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well , right now i just woke up from a mid day nap'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=doc.copy()\n",
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i]=np.array(list(vocab_processor.transform(data[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[  1,   2,   3, ...,   0,   0,   0],\n",
       "       [ 13,  14,  15, ...,   0,   0,   0],\n",
       "       [  4,  30,  31, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 32, 127, 123, ...,   0,   0,   0],\n",
       "       [  1,  13,  14, ...,   0,   0,   0],\n",
       "       [ 59,   4, 251, ...,   0,   0,   0]]),\n",
       "       array([[  1,  50, 255, ...,   0,   0,   0],\n",
       "       [  4, 184,  22, ...,   0,   0,   0],\n",
       "       [134, 261, 262, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [367,   1,  95, ...,   0,   0,   0],\n",
       "       [186, 106, 107, ...,   0,   0,   0],\n",
       "       [ 46, 166, 186, ...,   0,   0,   0]]),\n",
       "       array([[ 39,  66, 370, ...,   0,   0,   0],\n",
       "       [  4, 236, 376, ...,   0,   0,   0],\n",
       "       [383, 383, 383, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [496, 281, 538, ...,   0,   0,   0],\n",
       "       [140,   9, 397, ...,   0,   0,   0],\n",
       "       [186,  22, 125, ...,   0,   0,   0]]),\n",
       "       ...,\n",
       "       array([[   4,  193,  663, ...,    0,    0,    0],\n",
       "       [1211,   39, 9412, ...,    0,    0,    0],\n",
       "       [  39,  147,   71, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 247,  167,   39, ...,    0,    0,    0],\n",
       "       [ 166,  167,  746, ...,    0,    0,    0],\n",
       "       [  93,    4,  427, ...,    0,    0,    0]]),\n",
       "       array([[1100,  247, 1025, ...,    0,    0,    0],\n",
       "       [8644,    3,   13, ...,    0,    0,    0],\n",
       "       [ 111,  262, 5160, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   4,   47, 3989, ...,    0,    0,    0],\n",
       "       [  99,   44,  130, ...,    0,    0,    0],\n",
       "       [  13,    5,  696, ...,    0,    0,    0]]),\n",
       "       array([[ 278, 1172,   22, ...,    0,    0,    0],\n",
       "       [  32, 1942,  123, ...,    0,    0,    0],\n",
       "       [   4,   99,   44, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   4,   24,   25, ...,    0,    0,    0],\n",
       "       [1316, 1370,   51, ...,    0,    0,    0],\n",
       "       [  32,  186,  123, ...,    0,    0,    0]])], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.array(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[27, 51, 36, 23, 36, 54, 41, 15, 1, 25]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numsentences=[]\n",
    "for i in range(len(data)):\n",
    "    numsentences.append(len(data[i]))\n",
    "longistsent=max(numsentences)\n",
    "print(longistsent)\n",
    "numsentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    ess=data[i]\n",
    "    if len(ess)<129:\n",
    "        zeros=np.zeros([129-len(ess),149])\n",
    "        data[i]=np.concatenate((ess,zeros),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 149)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24781"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('/Users/ZP/Downloads/CS242/final project/glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(vocab_dict), 300))\n",
    "for word, i in vocab_dict.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.35089993e-01,  3.59070003e-01,  1.45300001e-01, -1.28289998e-01,\n",
       "       -5.57250008e-02,  4.01080012e-01, -9.40980017e-02,  2.30649993e-01,\n",
       "        6.72950000e-02, -1.94309998e+00,  2.86509991e-01, -3.13659990e-03,\n",
       "       -4.01000008e-02,  2.88390011e-01,  3.99289988e-02, -8.70169979e-03,\n",
       "       -5.51980019e-01,  1.09080002e-01,  1.52199998e-01,  1.69179998e-02,\n",
       "       -3.23860012e-02,  2.14800000e-01,  2.85769999e-01,  1.69420004e-01,\n",
       "       -3.66019994e-01,  5.65330014e-02, -3.63910012e-02,  5.48620000e-02,\n",
       "       -1.85980007e-01,  2.34190002e-01,  1.85440004e-01,  3.51570010e-01,\n",
       "       -2.84969985e-01,  1.27250001e-01, -7.76780009e-01,  4.64040004e-02,\n",
       "        2.24399999e-01,  2.63660014e-01,  6.99499995e-03, -1.83129996e-01,\n",
       "       -1.81970000e-01, -3.68809998e-01, -4.36020009e-02, -1.31569996e-01,\n",
       "        1.39960006e-01,  3.40290010e-01,  3.33579987e-01,  4.57659990e-01,\n",
       "       -2.21759994e-02,  4.24529999e-01, -5.87569997e-02, -1.77359998e-01,\n",
       "        1.80380002e-01, -1.71240002e-01, -2.02020004e-01, -4.61080018e-03,\n",
       "       -4.36180010e-02,  2.32020006e-01,  3.63640010e-01,  1.26200005e-01,\n",
       "        2.54460007e-01,  1.26029998e-01,  3.41989994e-01,  4.90209982e-02,\n",
       "       -2.04700008e-02, -2.28359997e-02,  3.16210002e-01,  9.19350013e-02,\n",
       "        7.88069963e-02,  6.07439987e-02,  5.61500005e-02, -1.60830006e-01,\n",
       "        1.91929996e-01,  1.69400007e-01, -8.59459979e-04, -2.67930001e-01,\n",
       "        2.77229995e-01,  3.76359999e-01, -2.92119989e-03, -1.42269999e-01,\n",
       "       -1.35790005e-01, -5.64909987e-02,  2.85210013e-01, -1.61190003e-01,\n",
       "        5.62440008e-02,  1.49959996e-01, -1.23520000e-02,  1.70379996e-01,\n",
       "        1.01859996e-03, -1.04709998e-01,  1.54970000e-02,  5.72430015e-01,\n",
       "       -2.36579999e-01, -8.95870030e-02, -1.82949994e-02,  1.25919998e-01,\n",
       "       -5.89569993e-02, -3.71050015e-02,  6.19400013e-03, -5.18999994e-03,\n",
       "        3.74240009e-03, -1.37419999e-01, -1.61060005e-01, -3.47799987e-01,\n",
       "       -7.38290027e-02,  2.57809997e-01,  6.23480007e-02,  2.64970005e-01,\n",
       "       -6.84880018e-02, -1.40699998e-01,  1.01920001e-01, -3.96100014e-01,\n",
       "       -6.86499998e-02, -2.09610000e-01,  4.41519991e-02,  2.75849998e-01,\n",
       "       -2.70040005e-01, -1.79769993e-01, -5.35940006e-02, -9.49229971e-02,\n",
       "       -1.26289994e-01, -6.98369965e-02,  8.47510025e-02,  5.30019999e-02,\n",
       "       -7.94269983e-03,  1.58230007e-01, -2.77920008e-01,  4.72849995e-01,\n",
       "        7.81669989e-02,  1.22960001e-01,  1.69180006e-01,  6.99599981e-02,\n",
       "        7.88950026e-02, -2.05259994e-01, -7.92270005e-02, -1.63100004e-01,\n",
       "       -7.28619993e-02,  1.82969999e-02, -7.42449984e-02,  5.59840024e-01,\n",
       "        1.44639999e-01, -2.46220008e-02,  9.19699967e-02,  1.05300002e-01,\n",
       "        6.49659988e-03,  6.11510016e-02,  1.27880007e-01, -7.33499974e-02,\n",
       "       -2.90479988e-01,  8.89939964e-02,  2.05699995e-01, -8.04269984e-02,\n",
       "       -3.00399996e-02,  4.34579998e-02,  1.92320004e-01, -3.43120005e-03,\n",
       "       -1.37630001e-01,  6.18979987e-03,  5.34669995e-01, -1.89040005e-01,\n",
       "        3.12399995e-02, -1.22500002e-01, -1.34259999e-01, -1.58989996e-01,\n",
       "       -2.18180001e-01,  6.11619987e-02,  6.19370006e-02, -2.22859997e-02,\n",
       "        2.69520015e-01,  3.99450004e-01,  3.29450011e-01,  1.67370006e-01,\n",
       "       -7.55999982e-01,  8.84869993e-02,  4.12559994e-02,  1.98789999e-01,\n",
       "       -1.35729998e-01, -1.60990000e-01, -9.00409967e-02,  2.75249988e-01,\n",
       "        1.17119998e-01,  4.54679988e-02,  4.34509993e-01, -1.10359997e-01,\n",
       "        2.96039999e-01, -1.88700005e-01, -4.26429994e-02,  1.32489994e-01,\n",
       "       -6.01760000e-02,  7.62929991e-02, -1.48250004e-02,  1.00100003e-01,\n",
       "       -1.32440001e-01,  1.04139999e-01, -1.33059993e-01, -3.55659992e-01,\n",
       "        1.98839996e-02, -2.64890003e-03, -5.19080013e-02, -2.58549988e-01,\n",
       "        9.62809980e-01,  6.61289990e-02,  2.62940000e-03,  9.55419987e-02,\n",
       "        1.29280001e-01,  2.38060001e-02,  5.45680001e-02,  1.14510000e-01,\n",
       "       -1.54600001e-03, -4.44860011e-01,  2.84159988e-01,  6.75650015e-02,\n",
       "        1.32630005e-01, -2.72190005e-01,  2.49100000e-01, -1.10210001e-01,\n",
       "       -1.34389997e-01,  8.10120031e-02,  2.30749995e-01, -4.93629985e-02,\n",
       "        2.34960005e-01, -6.77670017e-02, -4.90220010e-01, -5.16499989e-02,\n",
       "       -2.99130013e-04,  1.44290000e-01, -1.56240001e-01, -7.17770010e-02,\n",
       "       -2.17079997e-01, -2.30550006e-01, -1.56780005e-01, -3.57660018e-02,\n",
       "        6.81200027e-02, -8.06370005e-03, -4.10900004e-02,  3.33810002e-01,\n",
       "        1.95789993e-01,  3.64170000e-02, -4.55700010e-01,  2.63300002e-01,\n",
       "       -1.11690000e-01,  3.53839993e-01,  2.61240005e-01,  2.59570003e-01,\n",
       "       -7.18940020e-01,  9.66259986e-02,  6.43710000e-03,  2.17360005e-01,\n",
       "        6.17780015e-02,  1.25829995e-01,  3.48859996e-01, -2.85010010e-01,\n",
       "       -7.27299973e-02,  1.78450003e-01,  4.49559987e-01, -3.61750014e-02,\n",
       "        5.04090004e-02, -1.99290007e-01,  1.41730001e-02,  1.28099993e-01,\n",
       "       -1.27450004e-01, -9.44619998e-02,  2.39769995e-01,  1.13760002e-01,\n",
       "       -2.92710006e-01, -1.09559996e-02, -3.49599987e-01, -2.84729987e-01,\n",
       "        6.88840002e-02,  1.60479993e-01,  1.09399997e-01, -1.12250000e-01,\n",
       "       -1.38359994e-01,  2.21399993e-01,  4.04819995e-02,  6.70950022e-03,\n",
       "       -2.03160000e+00, -3.14919986e-02,  1.38699997e-03, -1.23779997e-01,\n",
       "       -1.51449993e-01, -4.97820005e-02,  8.62660035e-02,  2.41669998e-01,\n",
       "       -3.45969982e-02,  1.75899997e-01, -1.31329999e-03,  1.18249997e-01,\n",
       "        4.81380001e-02, -2.03759998e-01, -1.16609996e-02, -6.39579967e-02,\n",
       "       -1.06420003e-01, -2.36090004e-01,  1.88960001e-01,  2.41490006e-02,\n",
       "       -3.99359986e-02, -4.21000004e-01, -1.33399993e-01,  1.11900002e-01])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n",
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_essay(sentences,essay_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(sentence,sentence_length):\n",
    "    n=0\n",
    "    sentence.split('')\n",
    "    for word in sentence:\n",
    "        if word in emontial:\n",
    "            n=n+1\n",
    "    if n!=0:\n",
    "        #print(len(sentence))\n",
    "        while(len(sentence)<sentence_length):\n",
    "            index=sentence_length*np.random.rand(1)[0]\n",
    "            #print('raw index',index)\n",
    "            index=int(np.floor(index))-1\n",
    "            #print('index',index)\n",
    "            #print('length',len(sentence))\n",
    "            sentence.append(sentence[index])\n",
    "            #print('length after add',len(sentence))\n",
    "        s=' '.join(sentence)\n",
    "        return s\n",
    "    else:\n",
    "        return ''\n",
    "        \n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.floor(6*np.random.rand(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Well, right now I just woke up from a mid-day ...\n",
       "1    Well, here we go with the stream of consciousn...\n",
       "2    An open keyboard and buttons to push. The thin...\n",
       "3    I can't believe it!  It's really happening!  M...\n",
       "4    Well, here I go with the good old stream of co...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2435"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well , right now i just woke up from a mid day nap'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well , right now i just woke up from a mid day nap',\n",
       " \"it 's sort of weird , but ever since i moved to texas , i have had problems concentrating on things\",\n",
       " 'i remember starting my homework in 10th grade as soon as the clock struck 4 and not stopping until it was done',\n",
       " 'but when i moved here , the homework got a little more challenging and there was a lot more busy work , and so i decided not to spend hours doing it , and just getting by',\n",
       " \"but the thing was that i always paid attention in class and just plain out knew the stuff , and now that i look back , if i had really worked hard and stayed on track the last two years without getting lazy , i would have been a genius , but hey , that 's all good\",\n",
       " \"it 's too late to correct the past , but i do not really know how to stay focused n the future\",\n",
       " \"for me it would be easier there , but alas , i'm living at home under the watchful eye of my parents and a little nagging sister that just nags and nags and nags\",\n",
       " \"another thing is , is that it 's just a hassle to have to go all the way back to school to just to go to library to study\",\n",
       " 'do not get me wrong , i see where they are coming from and why they do not want me to move out , but i need to get away and be on my own',\n",
       " 'they have sheltered me so much and i do not have a worry in the world',\n",
       " 'the only thing that they ask me to do is keep my room clean and help out with the business once in a while , but i ca not even do that',\n",
       " 'but i got enough money from ut to live at a dorm or apartment next semester and i think i ll take advantage of that',\n",
       " 'but off that topic now , i went to sixth street last night and had a blast',\n",
       " 'i have not been there in so long',\n",
       " 'now i know why i love austin so much',\n",
       " 'when i lived in va , i used to go up to dc all the time and had a blast , but here , there are so many students running around at night',\n",
       " 'i just want to have some fun and i know that i am responsible enough to be able to have fun , but keep my priorities straight',\n",
       " 'when are you coming back',\n",
       " 'i just wish i could be treated like a responsible person for once , but my sister screwed that up for me',\n",
       " 'she went crazy the second she moved into college and messed up her whole college career by partying too much',\n",
       " \"and that 's the ultimate reason that they do not want me to go and have fun\",\n",
       " \"but i'm not little anymore , and they need to let me go and explore the world , but i m indian with indian culture , with indian values\",\n",
       " 'they go against having fun',\n",
       " 'i mean in the sense of meeting people or going out with people or partying or just plain having fun',\n",
       " \"my school is difficult already , but somehow i think that having more freedom will put more pressure on me to do better in school b c that 's what my parents and ultimately i expect of myself\",\n",
       " \"well it 's been fun writing , i do not know if you go anything out of this writing , but it helped me get some of my thoughts into order\",\n",
       " \"so i hope you had fun reading it and good luck ta 's\"]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
