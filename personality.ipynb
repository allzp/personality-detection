{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU,Input,TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.contrib import learn\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #AUTHID                                               TEXT cEXT  \\\n",
       "0  1997_504851.txt  Well, right now I just woke up from a mid-day ...    n   \n",
       "1  1997_605191.txt  Well, here we go with the stream of consciousn...    n   \n",
       "2  1997_687252.txt  An open keyboard and buttons to push. The thin...    n   \n",
       "3  1997_568848.txt  I can't believe it!  It's really happening!  M...    y   \n",
       "4  1997_688160.txt  Well, here I go with the good old stream of co...    y   \n",
       "\n",
       "  cNEU cAGR cCON cOPN  \n",
       "0    y    y    n    y  \n",
       "1    n    y    n    n  \n",
       "2    y    n    y    y  \n",
       "3    n    y    y    n  \n",
       "4    n    y    n    y  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('essays.csv',encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=df['TEXT']\n",
    "#raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>Charged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>march</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>august</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0   march      0             0        0     0    0         0         1   \n",
       "1  august      0             0        0     0    0         0         1   \n",
       "2     ago      0             0        0     0    0         0         0   \n",
       "3     mar      0             0        0     0    0         1         0   \n",
       "4     vie      0             0        0     0    0         0         0   \n",
       "\n",
       "   sadness  surprise  trust  Charged  \n",
       "0        0         0      0        1  \n",
       "1        0         0      0        1  \n",
       "2        0         0      0        0  \n",
       "3        0         0      0        1  \n",
       "4        0         0      0        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotionfile=pd.read_csv('Emotion_Lexicon.csv')\n",
    "emotionfile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emontial=emotionfile[emotionfile['Charged']!=0]['Words'].tolist()\n",
    "#emontial\n",
    "\n",
    "#emotionfile['sum']=emotionfile.sum(axis=1)\n",
    "#emontial=emotionfile[emotionfile['sum']!=0]['Words'].tolist()\n",
    "#emontial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, TREC=False):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s \", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have \", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not \", string)\n",
    "    string = re.sub(r\"\\'re\", \" are \", string)\n",
    "    string = re.sub(r\"\\'d\" , \" would \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will \", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "#   string = re.sub(r\"[^\\d\\.]\",\"\",string)\n",
    "#   string = re.sub(r\"[a-zA-Z]{4,}\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip() if TREC else string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawdata(filename):\n",
    "    df=pd.read_csv(filename,encoding='latin-1')\n",
    "    raw=df['TEXT']\n",
    "    EXT=pd.get_dummies(df['cEXT'])['y'].as_matrix()\n",
    "    NEU=pd.get_dummies(df['cNEU'])['y'].as_matrix()\n",
    "    AGR=pd.get_dummies(df['cAGR'])['y'].as_matrix()\n",
    "    CON=pd.get_dummies(df['cCON'])['y'].as_matrix()\n",
    "    OPN=pd.get_dummies(df['cOPN'])['y'].as_matrix()\n",
    "    EXT=EXT.reshape(len(EXT),1)\n",
    "    NEU=NEU.reshape(len(NEU),1)\n",
    "    AGR=AGR.reshape(len(AGR),1)\n",
    "    CON=CON.reshape(len(CON),1)\n",
    "    OPN=OPN.reshape(len(OPN),1)\n",
    "    ytrain=np.concatenate((EXT,NEU,AGR,CON,OPN),axis=1)\n",
    "    return raw,ytrain\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chargedwords(filename):\n",
    "    df=pd.read_csv(filename,encoding='latin-1')\n",
    "    emontial=df[df['Charged']!=0]['Words'].tolist()\n",
    "    return emontial\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_essay(raw,emontial):\n",
    "    doc=[]\n",
    "    sentence_length=[]\n",
    "    for i in raw:\n",
    "        essay=re.split(r'[.?]',i)\n",
    "        sentences=[]\n",
    "        for sent in essay:\n",
    "            sent=clean_str(sent)\n",
    "            sent=sent.split(' ')\n",
    "            if(len(sent)>150):\n",
    "                newsent=[]\n",
    "                splits=int(np.floor(len(sent)/20))\n",
    "                for index in range(splits):\n",
    "                            newsent.append(' '.join(sent[index*20:(index+1)*20]))\n",
    "                if len(sent)>splits*20:\n",
    "                            newsent.append(' '.join(sent[splits*20:]))\n",
    "                for s in newsent:\n",
    "                    n=0\n",
    "                    s=s.split(' ')\n",
    "                    for word in s:\n",
    "                        if word in emontial:\n",
    "                            n=n+1\n",
    "                    if n!=0:\n",
    "                        sentence_length.append(len(s))\n",
    "                        s=' '.join(s)\n",
    "                        sentences.append(s)\n",
    "                    \n",
    "            else:\n",
    "                n=0\n",
    "                for word in sent:\n",
    "                    if word in emontial:\n",
    "                        n=n+1\n",
    "                if n!=0:\n",
    "                    sentence_length.append(len(sent))\n",
    "                    sent=' '.join(sent)\n",
    "                    sentences.append(sent)    \n",
    "        doc.append(sentences)\n",
    "    return doc,max(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents(doc):\n",
    "    b=[]\n",
    "    for essay in doc:\n",
    "        for sent in essay:\n",
    "            b.append(sent)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "[raw,labels]=get_rawdata('essays.csv')\n",
    "\n",
    "emontial=get_chargedwords('Emotion_Lexicon.csv')\n",
    "\n",
    "[doc,max_wordNum]=clean_essay(raw,emontial)\n",
    "\n",
    "merged_essay=merge_sents(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get word-embedding matrix from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ff696d15cc32>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_wordNum)\n",
    "vocab_processor.fit_transform(merged_essay)\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_trained_embedding(dictionary,filename):\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    #print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(dictionary), 300))\n",
    "    for word, i in dictionary.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_pre_train=pre_trained_embedding(vocab_dict,'/Users/silver/ML/glove/glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to martix\n",
    "def text_to_martix(doc):\n",
    "    data=doc.copy()\n",
    "    for i in range(len(data)):\n",
    "        data[i]=np.array(list(vocab_processor.transform(data[i])))\n",
    "    data=np.array(data)\n",
    "    numsentences=[]\n",
    "    for i in range(len(data)):\n",
    "        numsentences.append(len(data[i]))\n",
    "    max_sentNum=max(numsentences)\n",
    "    for i in range(len(data)):\n",
    "        ess=data[i]\n",
    "        if len(ess)<max_sentNum:\n",
    "            zeros=np.zeros([129-len(ess),149])\n",
    "            data[i]=np.concatenate((ess,zeros),axis=0)\n",
    "    return data,max_sentNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "[data,max_sentNum]=text_to_martix(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reshape(data):\n",
    "    y=np.dstack(data)\n",
    "    y=np.rollaxis(y,-1)\n",
    "    return y\n",
    "#return a martix whose shape is (# of essays,# of sentences,# of words,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=data_reshape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_1 (Embedding)      (None, 129, 149, 300)     7692300   \n",
      "_________________________________________________________________\n",
      "Dropout_in (Dropout)         (None, 129, 149, 300)     0         \n",
      "_________________________________________________________________\n",
      "LSTM_1 (TimeDistributed)     (None, 129, 300)          721200    \n",
      "_________________________________________________________________\n",
      "Dropout_LSTM1 (Dropout)      (None, 129, 300)          0         \n",
      "_________________________________________________________________\n",
      "LSTM_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "Dropout_LSTM2 (Dropout)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 40)                12040     \n",
      "_________________________________________________________________\n",
      "Dense_out (Dense)            (None, 5)                 205       \n",
      "=================================================================\n",
      "Total params: 9,146,945\n",
      "Trainable params: 9,146,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 300\n",
    "modellstm = Sequential()\n",
    "modellstm.add(Embedding(len(vocab_dict),embedding_vecor_length,weights=[M_pre_train],activity_regularizer=regularizers.l2(1e-7)\n",
    "                       ,input_shape=(max_sentNum,max_wordNum),trainable=True,name='Embedding_1'))\n",
    "modellstm.add(Dropout(0.2,name='Dropout_in'))\n",
    "modellstm.add(TimeDistributed(LSTM(embedding_vecor_length,return_sequences=False),name='LSTM_1'))\n",
    "modellstm.add(Dropout(0.2,name='Dropout_LSTM1'))\n",
    "modellstm.add(LSTM(embedding_vecor_length,return_sequences=False,name='LSTM_2'))\n",
    "modellstm.add(Dropout(0.2,name='Dropout_LSTM2'))\n",
    "modellstm.add(Dense(40,activation='sigmoid',name='hidden_layer'))\n",
    "modellstm.add(Dense(5, activation='sigmoid',kernel_regularizer=regularizers.l2(1e-6),name='Dense_out'))\n",
    "modellstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modellstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "#don't run this line\n",
    "history=modellstm.fit(y,ytrain ,epochs=30, batch_size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
